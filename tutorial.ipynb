{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22fbaeb6",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Demo\n",
    "\n",
    "First, let's import libraries and test the custom ShariqQuest environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install gymnasium numpy matplotlib pygame torch seaborn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('1-custom-environment')\n",
    "sys.path.append('2-q-learning-agent')\n",
    "sys.path.append('3-dqn-agent')\n",
    "\n",
    "print(\"âœ“ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de8c5f",
   "metadata": {},
   "source": [
    "### Environment Specifications\n",
    "\n",
    "- **Grid Size:** 7Ã—7\n",
    "- **Agent Start:** Bottom-left (6, 0)\n",
    "- **Goal:** Top-right (0, 6)\n",
    "- **Traps:** 4 hell states\n",
    "- **Obstacles:** 5 barriers\n",
    "\n",
    "**Rewards:**\n",
    "- Goal: +10\n",
    "- Trap: -1\n",
    "- Step: -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca67bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the q-learning agent directory to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '2-q-learning-agent'))\n",
    "\n",
    "from padm_env import create_env\n",
    "\n",
    "# Create environment\n",
    "env = create_env(\n",
    "    goal_coordinates=(0, 6),\n",
    "    hell_state_coordinates=[(3, 2), (2, 3), (4, 4), (3, 5)],\n",
    "    obstacle_coordinates=[(3, 1), (3, 3), (4, 3), (5, 3), (1, 5)],\n",
    "    render_mode=False  # Disable Pygame window in notebook\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Environment created!\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ed775",
   "metadata": {},
   "source": [
    "### Test Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c084be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with random agent\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "max_steps = 50\n",
    "\n",
    "print(\"Starting position:\", state)\n",
    "\n",
    "while not done and steps < max_steps:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nâœ“ Episode finished!\")\n",
    "print(f\"  Steps: {steps}\")\n",
    "print(f\"  Total reward: {total_reward:.2f}\")\n",
    "print(f\"  Final state: {state}\")\n",
    "print(f\"  Goal reached: {info.get('goal_reached', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6e77b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Q-Learning Agent\n",
    "\n",
    "**Algorithm:** Tabular Q-Learning\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\cdot \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "Where:\n",
    "- Î± = learning rate\n",
    "- Î³ = discount factor\n",
    "- r = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning parameters\n",
    "learning_rate = 0.03\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((7, 7, 4))  # 7x7 grid, 4 actions\n",
    "\n",
    "print(\"Q-Learning Agent initialized!\")\n",
    "print(f\"Q-table shape: {q_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88187c59",
   "metadata": {},
   "source": [
    "### Training Loop (Mini Demo - 1000 episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf58c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini training (1000 episodes for demo)\n",
    "num_episodes = 1000\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0], state[1]])\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Q-learning update\n",
    "        old_q = q_table[state[0], state[1], action]\n",
    "        next_max_q = np.max(q_table[next_state[0], next_state[1]])\n",
    "        new_q = old_q + learning_rate * (reward + gamma * next_max_q - old_q)\n",
    "        q_table[state[0], state[1], action] = new_q\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    rewards_history.append(episode_reward)\n",
    "    \n",
    "    if (episode + 1) % 200 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-100:])\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Îµ: {epsilon:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Q-Learning training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed318212",
   "metadata": {},
   "source": [
    "### Visualize Q-Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards_history, alpha=0.3, label='Raw rewards')\n",
    "plt.plot(np.convolve(rewards_history, np.ones(100)/100, mode='valid'), \n",
    "         label='Moving average (100)', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final average reward: {np.mean(rewards_history[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d52cd",
   "metadata": {},
   "source": [
    "### Test Trained Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained agent\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "path = [tuple(state)]\n",
    "total_reward = 0\n",
    "\n",
    "while not done and len(path) < 50:\n",
    "    action = np.argmax(q_table[state[0], state[1]])  # Greedy\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    path.append(tuple(state))\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"âœ“ Test episode completed!\")\n",
    "print(f\"  Steps: {len(path) - 1}\")\n",
    "print(f\"  Total reward: {total_reward:.2f}\")\n",
    "print(f\"  Goal reached: {info.get('goal_reached', False)}\")\n",
    "print(f\"  Path: {path[:10]}...\" if len(path) > 10 else f\"  Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ac933",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Deep Q-Network (DQN)\n",
    "\n",
    "**Neural Network Architecture:**\n",
    "```\n",
    "Input (2) â†’ Dense(128, ReLU) â†’ Dense(128, ReLU) â†’ Output (4)\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- Experience Replay Buffer (50,000 transitions)\n",
    "- Target Network (updated every 20 episodes)\n",
    "- Huber Loss (Smooth L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cfd411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, no_states=2, no_actions=4):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(no_states, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, no_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = self.forward(state_t)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit=50000):\n",
    "        self.buffer = deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        return random.sample(self.buffer, n)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"âœ“ DQN model and replay buffer defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669e673",
   "metadata": {},
   "source": [
    "### Initialize DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23688c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.005\n",
    "gamma = 0.98\n",
    "buffer_limit = 50000\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize networks\n",
    "q_net = Qnet().to(device)\n",
    "q_target = Qnet().to(device)\n",
    "q_target.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Optimizer and memory\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "memory = ReplayBuffer(buffer_limit)\n",
    "\n",
    "print(\"âœ“ DQN agent initialized!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in q_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3dc753",
   "metadata": {},
   "source": [
    "### DQN Training Loop (Mini Demo - 500 episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c90057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini training (500 episodes for demo)\n",
    "num_episodes_dqn = 500\n",
    "epsilon_dqn = 0.08\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay_dqn = 0.995\n",
    "target_update_freq = 20\n",
    "\n",
    "dqn_rewards_history = []\n",
    "\n",
    "for episode in range(num_episodes_dqn):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = q_net.sample_action(state, epsilon_dqn)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition\n",
    "        memory.put((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Train if enough samples\n",
    "        if memory.size() > 200:\n",
    "            # Sample mini-batch\n",
    "            mini_batch = memory.sample(batch_size)\n",
    "            \n",
    "            states = torch.FloatTensor([t[0] for t in mini_batch]).to(device)\n",
    "            actions = torch.LongTensor([t[1] for t in mini_batch]).to(device)\n",
    "            rewards = torch.FloatTensor([t[2] for t in mini_batch]).to(device)\n",
    "            next_states = torch.FloatTensor([t[3] for t in mini_batch]).to(device)\n",
    "            dones = torch.FloatTensor([t[4] for t in mini_batch]).to(device)\n",
    "            \n",
    "            # Compute Q-values\n",
    "            q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute target Q-values\n",
    "            with torch.no_grad():\n",
    "                max_next_q = q_target(next_states).max(1)[0]\n",
    "                target_q = rewards + gamma * max_next_q * (1 - dones)\n",
    "            \n",
    "            # Compute loss and update\n",
    "            loss = F.smooth_l1_loss(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    # Update target network\n",
    "    if (episode + 1) % target_update_freq == 0:\n",
    "        q_target.load_state_dict(q_net.state_dict())\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon_dqn = max(epsilon_end, epsilon_dqn * epsilon_decay_dqn)\n",
    "    dqn_rewards_history.append(episode_reward)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(dqn_rewards_history[-100:])\n",
    "        print(f\"Episode {episode + 1}/{num_episodes_dqn} | Avg Reward: {avg_reward:.2f} | Îµ: {epsilon_dqn:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ DQN training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c1389",
   "metadata": {},
   "source": [
    "### Visualize DQN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a773410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DQN training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dqn_rewards_history, alpha=0.3, label='Raw rewards')\n",
    "plt.plot(np.convolve(dqn_rewards_history, np.ones(50)/50, mode='valid'), \n",
    "         label='Moving average (50)', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final average reward: {np.mean(dqn_rewards_history[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84f687",
   "metadata": {},
   "source": [
    "### Test Trained DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e396dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained DQN agent\n",
    "q_net.eval()\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "path = [tuple(state)]\n",
    "total_reward = 0\n",
    "\n",
    "while not done and len(path) < 50:\n",
    "    action = q_net.sample_action(state, epsilon=0.0)  # Greedy\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    path.append(tuple(state))\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"âœ“ Test episode completed!\")\n",
    "print(f\"  Steps: {len(path) - 1}\")\n",
    "print(f\"  Total reward: {total_reward:.2f}\")\n",
    "print(f\"  Goal reached: {info.get('goal_reached', False)}\")\n",
    "print(f\"  Path: {path[:10]}...\" if len(path) > 10 else f\"  Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468b595",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Results Visualization\n",
    "\n",
    "### Load Pre-trained Models (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Q-table\n",
    "try:\n",
    "    q_table_trained = np.load('2-q-learning-agent/q_table.npy')\n",
    "    print(\"âœ“ Loaded trained Q-table from file\")\n",
    "    print(f\"  Shape: {q_table_trained.shape}\")\n",
    "    print(f\"  Non-zero values: {np.count_nonzero(q_table_trained)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Pre-trained Q-table not found. Using freshly trained one.\")\n",
    "    q_table_trained = q_table\n",
    "\n",
    "# Load pre-trained DQN model\n",
    "try:\n",
    "    q_net_trained = Qnet().to(device)\n",
    "    q_net_trained.load_state_dict(torch.load('3-dqn-agent/dqn.pth', map_location=device))\n",
    "    q_net_trained.eval()\n",
    "    print(\"\\nâœ“ Loaded trained DQN model from file\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš  Pre-trained DQN model not found. Using freshly trained one.\")\n",
    "    q_net_trained = q_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a4cf2",
   "metadata": {},
   "source": [
    "### Compare Q-Learning vs DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both methods\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Q-Learning performance\n",
    "axes[0].plot(rewards_history, alpha=0.3)\n",
    "axes[0].plot(np.convolve(rewards_history, np.ones(100)/100, mode='valid'), linewidth=2)\n",
    "axes[0].set_title('Q-Learning Training')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# DQN performance\n",
    "axes[1].plot(dqn_rewards_history, alpha=0.3)\n",
    "axes[1].plot(np.convolve(dqn_rewards_history, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "axes[1].set_title('DQN Training')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Total Reward')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Performance Comparison:\")\n",
    "print(f\"Q-Learning final avg reward: {np.mean(rewards_history[-100:]):.2f}\")\n",
    "print(f\"DQN final avg reward: {np.mean(dqn_rewards_history[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae29da",
   "metadata": {},
   "source": [
    "### Visualize Q-Values (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523898f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Visualize max Q-values across the grid\n",
    "max_q_values = np.max(q_table_trained, axis=2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(max_q_values, annot=True, fmt='.2f', cmap='viridis', \n",
    "            cbar_kws={'label': 'Max Q-Value'})\n",
    "plt.title('Q-Learning: Max Q-Values per State')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eed88c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Custom Environment:** Created a 7Ã—7 gridworld with traps and obstacles\n",
    "2. **Q-Learning:** Tabular RL method â€” simple but effective for small state spaces\n",
    "3. **DQN:** Neural network approximation â€” scales to larger/continuous spaces\n",
    "\n",
    "**Key Takeaways:**\n",
    "- âœ… Q-Learning converges quickly for small grids\n",
    "- âœ… DQN requires more episodes but generalizes better\n",
    "- âœ… Epsilon-greedy exploration is crucial\n",
    "- âœ… Reward shaping affects learning speed\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different hyperparameters\n",
    "- Implement Double DQN or Dueling DQN\n",
    "- Scale to larger environments\n",
    "- Add stochastic transitions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Resources\n",
    "\n",
    "- [Full Project Repository](https://github.com/muk0644/autonomous-agent-q-learning-dqn)\n",
    "- [Sutton & Barto: Reinforcement Learning Book](http://incompleteideas.net/book/the-book.html)\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Shariq Khan  \n",
    "**Contact:** engr.m.shariqkhan@gmail.com  \n",
    "**GitHub:** [@muk0644](https://github.com/muk0644)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
